{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTir8avJAEvEJ5dpJ6BTaX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nskoushik26276/Product-Pulse-AI/blob/main/product_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9ZG_INfSHAB"
      },
      "outputs": [],
      "source": [
        "!pip install pytrends transformers sentencepiece scikit-learn beautifulsoup4 requests pandas matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googlesearch-python pytrends transformers scikit-learn beautifulsoup4 requests --quiet\n",
        "\n",
        "import time\n",
        "import re\n",
        "import math\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pytrends.request import TrendReq\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from googlesearch import search\n",
        "\n",
        "# Initialize sentiment analyzer (Hugging Face)\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# -------------------------\n",
        "# Utilities: web helpers\n",
        "# -------------------------\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36\"}\n",
        "\n",
        "def safe_get(url, timeout=8):\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "        if r.status_code == 200:\n",
        "            return r.text\n",
        "        return \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_blocks(html, min_len=40, max_len=600):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    for s in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"svg\", \"form\", \"button\"]):\n",
        "        s.extract()\n",
        "    text = soup.get_text(separator=\"\\n\")\n",
        "    parts = [p.strip() for p in re.split(r'\\n{2,}', text) if min_len <= len(p.strip()) <= max_len]\n",
        "    return parts\n",
        "\n",
        "# -------------------------\n",
        "# Auto-collect reviews from top search results\n",
        "# -------------------------\n",
        "PREFERRED_DOMAINS = [\"amazon.\", \"flipkart.\", \"walmart.\", \"reddit.\", \"trustpilot.\", \"bestbuy.\", \"consumerreports.\", \"thewirecutter.\", \"cnet.\", \"gsmarena.\", \"techradar.\", \"pcmag.\", \"producthunt.\", \"play.google\", \"apps.apple\"]\n",
        "\n",
        "def auto_collect_reviews(product_name, max_pages=8, per_page_limit=30):\n",
        "    queries = [\n",
        "        f\"{product_name} reviews\",\n",
        "        f\"{product_name} review\",\n",
        "        f\"{product_name} complaints\",\n",
        "        f\"buy {product_name}\",\n",
        "        f\"{product_name} customer reviews\",\n",
        "    ]\n",
        "    seen_urls = set()\n",
        "    collected = []\n",
        "    likely_product_found = False\n",
        "\n",
        "    for q in queries:\n",
        "        try:\n",
        "            for url in search(q, num=max_pages, stop=max_pages, pause=1.0):\n",
        "                if url in seen_urls:\n",
        "                    continue\n",
        "                seen_urls.add(url)\n",
        "                lower = url.lower()\n",
        "                if any(dom in lower for dom in [\"amazon.\", \"flipkart.\", \"walmart.\", \"bestbuy.\", \"producthunt.\", \"play.google\", \"apps.apple\"]):\n",
        "                    likely_product_found = True\n",
        "                html = safe_get(url)\n",
        "                if not html:\n",
        "                    continue\n",
        "                blocks = extract_text_blocks(html)\n",
        "                scored = []\n",
        "                for b in blocks:\n",
        "                    score = 0\n",
        "                    lower_b = b.lower()\n",
        "                    if any(w in lower_b for w in [\"review\", \"rating\", \"stars\", \"customer\", \"customers\", \"complaint\", \"issue\", \"problem\", \"love\", \"hate\"]):\n",
        "                        score += 2\n",
        "                    if len(b.split()) > 8:\n",
        "                        score += 1\n",
        "                    scored.append((score, b))\n",
        "                scored.sort(key=lambda x: x[0], reverse=True)\n",
        "                selected = [b for s,b in scored if s>0][:per_page_limit]\n",
        "                if not selected:\n",
        "                    selected = blocks[:min(len(blocks), per_page_limit)]\n",
        "                for blk in selected:\n",
        "                    if blk and blk not in collected:\n",
        "                        collected.append(blk)\n",
        "                time.sleep(0.8)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return collected[:400], likely_product_found\n",
        "\n",
        "# -------------------------\n",
        "# Google Trends helpers\n",
        "# -------------------------\n",
        "def get_trend_data(keyword, timeframe='today 12-m', geo=''):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        pytrends.build_payload([keyword], timeframe=timeframe, geo=geo)\n",
        "        data = pytrends.interest_over_time()\n",
        "        if data.empty:\n",
        "            return None\n",
        "        data = data.reset_index()\n",
        "        if 'isPartial' in data.columns:\n",
        "            data = data.drop(columns=['isPartial'])\n",
        "        return data\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def compute_trend_slope(df):\n",
        "    if df is None or df.shape[0] < 3:\n",
        "        return 0.0\n",
        "    x = np.arange(len(df)).reshape(-1,1)\n",
        "    y = df.iloc[:,1].values.reshape(-1,1)\n",
        "    lr = LinearRegression().fit(x, y)\n",
        "    slope = float(lr.coef_[0][0])\n",
        "    return slope\n",
        "\n",
        "def get_top_regions(keyword, timeframe='today 12-m'):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        pytrends.build_payload([keyword], timeframe=timeframe)\n",
        "        df_regions = pytrends.interest_by_region(resolution='COUNTRY', inc_low_vol=True, inc_geo_code=False)\n",
        "        if df_regions is None or df_regions.empty:\n",
        "            return []\n",
        "        df_regions = df_regions.sort_values(by=keyword, ascending=False)\n",
        "        top = df_regions[df_regions[keyword] > 0].head(10)\n",
        "        return list(top.index[:5])\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# -------------------------\n",
        "# Review analysis\n",
        "# -------------------------\n",
        "def analyze_reviews(review_texts, top_k_keywords=8):\n",
        "    if not review_texts:\n",
        "        return {\"count\": 0, \"avg_sentiment_score\": 0.0, \"pos_pct\": 0.0, \"neg_pct\": 0.0, \"top_negative_keywords\": []}\n",
        "    sentiments = []\n",
        "    for txt in review_texts:\n",
        "        try:\n",
        "            out = sentiment_analyzer(txt[:512])\n",
        "            label = out[0]['label']\n",
        "            score = out[0]['score']\n",
        "            signed = score if label == \"POSITIVE\" else -score\n",
        "            sentiments.append(signed)\n",
        "        except Exception:\n",
        "            sentiments.append(0.0)\n",
        "    avg_score = float(np.mean(sentiments))\n",
        "    pos_pct = float(sum(1 for s in sentiments if s > 0)/len(sentiments))*100\n",
        "    neg_pct = float(sum(1 for s in sentiments if s < 0)/len(sentiments))*100\n",
        "\n",
        "    neg_texts = [t for s,t in zip(sentiments, review_texts) if s < 0]\n",
        "    top_neg_keywords = []\n",
        "    if neg_texts:\n",
        "        try:\n",
        "            vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=2000)\n",
        "            X = vect.fit_transform(neg_texts)\n",
        "            sums = np.asarray(X.sum(axis=0)).ravel()\n",
        "            terms = np.array(vect.get_feature_names_out())\n",
        "            top_idx = sums.argsort()[::-1][:top_k_keywords]\n",
        "            top_neg_keywords = list(terms[top_idx])\n",
        "        except Exception:\n",
        "            top_neg_keywords = []\n",
        "    return {\"count\": len(review_texts), \"avg_sentiment_score\": avg_score, \"pos_pct\": pos_pct, \"neg_pct\": neg_pct, \"top_negative_keywords\": top_neg_keywords}\n",
        "\n",
        "# -------------------------\n",
        "# Success scoring & reasoning\n",
        "# -------------------------\n",
        "def compute_success_score(trend_slope, avg_sentiment, review_count, price_competitiveness_score):\n",
        "    slope_score = (1 / (1 + math.exp(-10*(trend_slope/5)))) if trend_slope != 0 else 0.5\n",
        "    sentiment_score = (avg_sentiment + 1) / 2\n",
        "    review_score = min(review_count / 100.0, 1.0)\n",
        "    price_score = price_competitiveness_score\n",
        "    score = 0.4*slope_score + 0.35*sentiment_score + 0.15*review_score + 0.1*price_score\n",
        "    components = {\"slope_score\": slope_score, \"sentiment_score\": sentiment_score, \"review_score\": review_score, \"price_score\": price_score}\n",
        "    return score, components\n",
        "\n",
        "def generate_failure_reason_and_mods(refined_input, analysis, components):\n",
        "    reasons = []\n",
        "    suggestions = []\n",
        "    if analysis['count'] == 0:\n",
        "        reasons.append(\"Insufficient real-user feedback or reviews.\")\n",
        "        suggestions.append(\"Collect early user trials, pilot test, gather feedback.\")\n",
        "    if analysis['neg_pct'] > 40:\n",
        "        reasons.append(\"High negative sentiment.\")\n",
        "        suggestions.append(\"Address top complaints: \" + \", \".join(analysis['top_negative_keywords'][:5]))\n",
        "    if components['slope_score'] < 0.4:\n",
        "        reasons.append(\"Low trend interest.\")\n",
        "        suggestions.append(\"Refine positioning or target trending adjacent categories.\")\n",
        "    if components['price_score'] < 0.4:\n",
        "        reasons.append(\"Price competitiveness is low.\")\n",
        "        suggestions.append(\"Consider value-based pricing or cheaper variants.\")\n",
        "    if components['review_score'] < 0.2:\n",
        "        reasons.append(\"Few reviews; adoption risk.\")\n",
        "        suggestions.append(\"Run early campaigns to gather reviews.\")\n",
        "    return reasons, suggestions\n",
        "\n",
        "def generate_success_strategy(product_name, product_desc, top_regions, trend_slope, sentiment_analysis):\n",
        "    slope = trend_slope\n",
        "    if slope > 0.6:\n",
        "        timeline = \"1-4 weeks (rapid seeding)\"\n",
        "    elif slope > 0.15:\n",
        "        timeline = \"2-8 weeks (targeted campaigns)\"\n",
        "    else:\n",
        "        timeline = \"1-3 months (build awareness)\"\n",
        "    channels = [\"Social media\",\"Influencer partnerships\"] if sentiment_analysis['pos_pct']>60 else [\"Content marketing\",\"Niche forums\",\"Paid search\"]\n",
        "    regions = top_regions[:3] if top_regions else [\"US\",\"India\",\"UK\"]\n",
        "    mods = []\n",
        "    if sentiment_analysis['neg_pct']>30:\n",
        "        mods.append(\"Fix top negative areas: \" + \", \".join(sentiment_analysis['top_negative_keywords'][:5]))\n",
        "    strategy = {\n",
        "        \"timeline\": timeline,\n",
        "        \"channels\": channels,\n",
        "        \"regions\": regions,\n",
        "        \"key_recommendations\": [\n",
        "            \"Create 5 short-form videos per top region\",\n",
        "            \"Run small A/B tests for creative\",\n",
        "            \"Collect early feedback and iterate on top pain points\"\n",
        "        ] + mods\n",
        "    }\n",
        "    return strategy\n",
        "\n",
        "# -------------------------\n",
        "# AUTO-MODE main function\n",
        "# -------------------------\n",
        "def run_product_assessor_auto():\n",
        "    print(\"=== TrendScope AI â€” Auto Product Success Predictor (Auto Review Mode) ===\")\n",
        "    while True:\n",
        "        product_name = input(\"\\nProduct name (or 'quit' to exit): \").strip()\n",
        "        if not product_name:\n",
        "            print(\"Please enter a product name.\")\n",
        "            continue\n",
        "        if product_name.lower() in ['quit','exit','q']:\n",
        "            print(\"Exiting.\")\n",
        "            break\n",
        "\n",
        "        # -------------------------\n",
        "        # Auto-collect reviews (check idea availability first)\n",
        "        # -------------------------\n",
        "        print(\"\\nðŸ”Ž Checking if similar product already exists online...\")\n",
        "        _, product_found = auto_collect_reviews(product_name, max_pages=4, per_page_limit=5)\n",
        "        if product_found:\n",
        "            print(\"âš ï¸ Idea already taken / similar product exists! Please try a different product idea.\")\n",
        "            continue  # go back to product name prompt\n",
        "        else:\n",
        "          print(\"ðŸ†• This is a unique product idea!\\n\")\n",
        "        # proceed if idea not found\n",
        "        product_desc = input(\"Short product description / uses (1-2 sentences):\\n> \").strip()\n",
        "        target_users = input(\"Target users (comma-separated, optional):\\n> \").strip()\n",
        "        price_input = input(\"Is the product priced competitively? (0..1) [Enter if unknown]:\\n> \").strip()\n",
        "        try:\n",
        "            price_score = float(price_input) if price_input else 0.6\n",
        "            price_score = max(0.0, min(1.0, price_score))\n",
        "        except:\n",
        "            price_score = 0.6\n",
        "\n",
        "        print(\"\\nðŸ”Ž Collecting detailed reviews from web...\")\n",
        "        reviews, _ = auto_collect_reviews(product_name, max_pages=8)\n",
        "        print(f\"Collected {len(reviews)} review-like text blocks.\")\n",
        "\n",
        "        # Trend data\n",
        "        print(\"\\nFetching Google Trends data...\")\n",
        "        trend_df = get_trend_data(product_name)\n",
        "        slope = compute_trend_slope(trend_df) if trend_df is not None else 0.0\n",
        "        top_regions = get_top_regions(product_name)\n",
        "        print(f\"Trend slope: {slope:.4f}, top regions: {top_regions[:5]}\")\n",
        "\n",
        "        # Reviews analysis\n",
        "        print(\"\\nAnalyzing reviews and sentiment...\")\n",
        "        review_analysis = analyze_reviews(reviews)\n",
        "        print(\"Avg sentiment:\", review_analysis['avg_sentiment_score'], \"Positive%:\", review_analysis['pos_pct'], \"Negative%:\", review_analysis['neg_pct'])\n",
        "\n",
        "        # Compute success score\n",
        "        score, comps = compute_success_score(slope, review_analysis['avg_sentiment_score'], review_analysis['count'], price_score)\n",
        "        print(\"\\n=== Prediction Summary ===\")\n",
        "        print(f\"Composite success score: {score*100:.1f}%\")\n",
        "\n",
        "        threshold = 0.5\n",
        "        if score < threshold:\n",
        "            print(\"\\nPrediction: Likely unsuccessful.\")\n",
        "            reasons, suggestions = generate_failure_reason_and_mods({\"name\": product_name, \"desc\": product_desc, \"users\": target_users}, review_analysis, comps)\n",
        "            print(\"\\nReasons:\")\n",
        "            for r in reasons: print(\"-\", r)\n",
        "            print(\"\\nSuggestions:\")\n",
        "            for s in suggestions: print(\"-\", s)\n",
        "        else:\n",
        "            print(\"\\nPrediction: Likely to succeed (with proper strategy).\")\n",
        "            strategy = generate_success_strategy(product_name, product_desc, top_regions, slope, review_analysis)\n",
        "            print(\"\\nRecommended launch timeline:\", strategy['timeline'])\n",
        "            print(\"Top regions:\", \", \".join(strategy['regions']))\n",
        "            print(\"Best channels:\")\n",
        "            for ch in strategy['channels']: print(\"-\", ch)\n",
        "            print(\"Key action items:\")\n",
        "            for k in strategy['key_recommendations']: print(\"-\", k)\n",
        "            if review_analysis['top_negative_keywords']:\n",
        "                print(\"\\nSuggested product modifications (based on negative feedback):\")\n",
        "                for kw in review_analysis['top_negative_keywords'][:8]:\n",
        "                    print(\"-\", kw)\n",
        "            print(\"\\nProduct description:\")\n",
        "            print(product_desc)\n",
        "        again = input(\"\\nEvaluate another product? (yes/no): \").strip().lower()\n",
        "        if not again.startswith('y'):\n",
        "            print(\"Done. Exiting.\")\n",
        "            break\n",
        "\n",
        "# Run the auto-mode assessor\n",
        "run_product_assessor_auto()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDDcYFhlmphy",
        "outputId": "cf11fb62-8444-43fd-c760-4dfa92e28296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TrendScope AI â€” Auto Product Success Predictor (Auto Review Mode) ===\n",
            "\n",
            "Product name (or 'quit' to exit): kitchen robot\n",
            "\n",
            "ðŸ”Ž Checking if similar product already exists online...\n",
            "ðŸ†• This is a unique product idea!\n",
            "\n",
            "Short product description / uses (1-2 sentences):\n",
            "> the kitchen robot is a robot which first observes the work in the kitchen i,e food recipies, utensils cleaning and also kitchen platform,stove cleaning and also cleans the dirt on the kitchen walls\n",
            "Target users (comma-separated, optional):\n",
            "> house maids, house wives\n",
            "Is the product priced competitively? (0..1) [Enter if unknown]:\n",
            "> \n",
            "\n",
            "ðŸ”Ž Collecting detailed reviews from web...\n",
            "Collected 0 review-like text blocks.\n",
            "\n",
            "Fetching Google Trends data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trend slope: 0.9339, top regions: ['Guam', 'Haiti', 'Slovenia', 'Czechia', 'Slovakia']\n",
            "\n",
            "Analyzing reviews and sentiment...\n",
            "Avg sentiment: 0.0 Positive%: 0.0 Negative%: 0.0\n",
            "\n",
            "=== Prediction Summary ===\n",
            "Composite success score: 58.1%\n",
            "\n",
            "Prediction: Likely to succeed (with proper strategy).\n",
            "\n",
            "Recommended launch timeline: 1-4 weeks (rapid seeding)\n",
            "Top regions: Guam, Haiti, Slovenia\n",
            "Best channels:\n",
            "- Content marketing\n",
            "- Niche forums\n",
            "- Paid search\n",
            "Key action items:\n",
            "- Create 5 short-form videos per top region\n",
            "- Run small A/B tests for creative\n",
            "- Collect early feedback and iterate on top pain points\n",
            "\n",
            "Product description:\n",
            "the kitchen robot is a robot which first observes the work in the kitchen i,e food recipies, utensils cleaning and also kitchen platform,stove cleaning and also cleans the dirt on the kitchen walls\n",
            "\n",
            "Evaluate another product? (yes/no): no\n",
            "Done. Exiting.\n"
          ]
        }
      ]
    }
  ]
}